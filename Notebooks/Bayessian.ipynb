{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook Bayessian approach to denoising images is preformed\n",
    "\n",
    "Idea is taken from the [post](https://blog.statsbot.co/probabilistic-graphical-models-tutorial-d855ba0107d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Idea\n",
    "\n",
    "The idea is the following: having $M \\cdot N$ size image we assume that pixels values are observed variables of Markov process and corresponding $M \\cdot N$ values of denoised image are hidden variables.\n",
    "\n",
    "For simplicity we assume that image is binary, i.e. every pixel could be either -1, either 1.\n",
    "\n",
    "We also assume that observed variable is connected only with corresponding hidden variable, but hidden variable is connected to its 4 neighbours. In this way we reflect the idea that pixel value should be close to the value of its neighbours.\n",
    "\n",
    "If we denote hidden variables as $Y$ and observed variables as $X$, MAP estimation of $Y$ will be:\n",
    "\n",
    "$$ \\hat{Y} = \\underset{Y}{\\operatorname{argmax}}[P(Y|X)] = \n",
    "\\underset{Y}{\\operatorname{argmax}}[ log(P(Y|X) ]  = \n",
    "\\underset{Y}{\\operatorname{argmax}}[log(P(Y, X) - log(X) ] = \n",
    "\\underset{Y}{\\operatorname{argmax}}[log(P(X, Y))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we model probability $P(X, Y)$ according to our model as:\n",
    "\n",
    "$$ P(X, Y) = const \\cdot \\prod_{i, j} \\Omega (X_{ij}, Y_{ij}) \\prod_{(i,j), (k,l)} \\Psi (Y_{ij}, Y_{kl}), $$\n",
    "where $\\Omega (X_{ij}, Y_{ij})$ reflects connection between hidden state and observed state. It should be bigger, when they are equal and lower, when the are different. Thus we can model it as:\n",
    "\n",
    "$$ \\Omega (X_{ij}, Y_{ij}) = \\exp (w_1 \\cdot X_{ij} Y_{ij} ) .$$\n",
    "\n",
    "If $X_{ij}, Y_{ij}$ are the same, their product is 1, thus exponent is equal to $\\exp(w_1)$. Otherwise, it is equal to $\\exp(-w_1)$.\n",
    "\n",
    "In the same manner we define $\\Psi (Y_{ij}, Y_{kl})$ as:\n",
    "\n",
    "$$ \\Psi (Y_{ij}, Y_{kl}) = \\exp(w_2 \\cdot Y_{ij} Y_{kl}) .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, our hidden state estimation will look like:\n",
    "\n",
    "$$ \\hat{Y} = \\underset{Y}{\\operatorname{argmax}} \\left[\n",
    "\\sum_{i, j} w_1 X_{ij} Y_{ij} + \\sum_{(i,j), (k,l)} w_2 Y_{i,j} Y_{k, l} \\right]  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data\n",
    "\n",
    "As simple example we will use the following noisy image:\n",
    "\n",
    "<img src=\"example/noisy.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Use [post](https://towardsdatascience.com/linear-programming-and-discrete-optimization-with-python-using-pulp-449f3c5f6e99) for linear programming library.\n",
    "\n",
    "2) Try to do it for continious -1, 1.\n",
    "\n",
    "3) Find a way to estimate $w_1, w_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
